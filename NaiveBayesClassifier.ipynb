{"cells":[{"cell_type":"markdown","metadata":{"id":"XGPIIJlJweUt"},"source":["# Lab 1 - Naive Bayes Classifier"]},{"cell_type":"markdown","metadata":{"id":"t1O-SSLytW2k"},"source":["## Introduction\n","During the past three weeks, you learned a couple of essential notions ant theorems. One of them is Bayes theorem.\n","\n","One of its applications is **Naive Bayes classifier**, which is a probabilistic classifier whose aim is to determine which class some observation probably belongs by using the Bayes formula:\n","$$\\mathsf{P}(\\mathrm{class}\\mid \\mathrm{observation})=\\frac{\\mathsf{P}(\\mathrm{observation}\\mid\\mathrm{class})\\mathsf{P}(\\mathrm{class})}{\\mathsf{P}(\\mathrm{observation})}$$\n","\n","Under the strong independence assumption, one can calculate $\\mathsf{P}(\\mathrm{observation} \\mid \\mathrm{class})$ as\n","$$\\mathsf{P}(\\mathrm{observation}) = \\prod_{i=1}^{n} \\mathsf{P}(\\mathrm{feature}_i),$$\n","where $n$ is the total number of features describing a given observation. Thus, $\\mathsf{P}(\\mathrm{class}|\\mathrm{observation})$ now can be calculated as\n","\n","$$\\mathsf{P}(\\mathrm{class} \\mid \\mathrm{\\mathrm{observation}}) = \\mathsf{P}(\\mathrm{class})\\times \\prod_{i=1}^{n}\\frac{\\mathsf{P}(\\mathrm{feature}_i\\mid \\mathrm{class})}{\\mathsf{P}(\\mathrm{feature}_i)}$$\n","\n","For more detailed explanation, you can check [this link](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VzV_ykevv9Tp"},"source":["## Data  description\n","\n","* **1 - discrimination**\n","This data set consists of tweets that have discriminatory (sexism or racism) messages or of tweets that are of neutral mood. The task is to determine whether a given tweet has discriminatory mood or does not.\n","\n","Data set consists of two files: *train.csv* and *test.csv*. The first one to find the probabilities distributions for each of the features, while the second one is for checking how well classifier works.\n"]},{"cell_type":"markdown","metadata":{"id":"ASP_dWuj9t0l"},"source":["##Implementation"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Cv-5_R7D6bTP"},"outputs":[],"source":["import pandas as pd\n","import string"]},{"cell_type":"markdown","metadata":{"id":"JvZsfHRL6JOA"},"source":["### Data pre-processing\n","* Read the *.csv* data files with *pandas* package. This package will also provide you with a nice interface for data processing even within the classifier implementation.\n","* Сlear your data from punctuation or other unneeded symbols.\n","* Clear you data from stop words. You don’t want words as is, and, or etc. to affect your probabilities distributions, so it is a wise decision to get rid of them. Find list of stop words in the cms under the lab task.\n","* Represent each test message as its bag-of-words. [Here](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) you can find general introduction to the bag-of-words model and examples on to create it."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"XeSKb6pQ9DoL"},"outputs":[],"source":["def process_data(data_file: str) -> tuple:\n","   \"\"\"\n","  Function for data processing and split it into X and y sets.\n","  :param data_file: str - train data\n","  :return: pd.DataFrame|list, pd.DataFrame|list - X and y data frames or lists\n","   \"\"\"\n","   if \"test\" in data_file:\n","      df = pd.read_csv(data_file)\n","\n","      return df[\"tweet\"].values, df[\"label\"].values\n","\n","   def filter_tweets(tweet: str) -> str:\n","      \"\"\"Filter all tweets from puntuation and stopwords\"\"\"\n","      tweet_array = list()\n","      for word in tweet.split():\n","         word = word.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n","         if word not in stop_words:\n","            tweet_array.append(word)\n","      return \" \".join(tweet_array)\n","\n","   def tweets_to_features(tweet: str) -> str:\n","      tweet_dict = dict()\n","      for word in tweet.split():\n","         tweet_dict[word] = tweet.count(word)\n","      return tweet_dict\n","\n","   df = pd.read_csv(data_file)\n","\n","   with open(\"./data//stop_words.txt\", mode=\"r\", encoding=\"ascii\") as stop_words_file:\n","      stop_words = [word.strip(\"\\n\") for word in stop_words_file.readlines()]\n","\n","   df[\"tweet\"] = df[\"tweet\"].apply(filter_tweets)\n","\n","   tweets = df['tweet'].apply(tweets_to_features).values\n","   labels = df['label'].values\n","\n","   return tweets, labels\n"]},{"cell_type":"markdown","metadata":{"id":"Ntmhf9VA9u0d"},"source":["*   If you need to implement some additional methods, feel free to do it."]},{"cell_type":"markdown","metadata":{"id":"95OjORbi9zAW"},"source":["### Implementation\n","Implement each method of the BayesianClassifier \n","created according to its description."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"rx2-IVxy-R99"},"outputs":[],"source":["class BayesianClassifier:\n","    \"\"\"\n","    Implementation of Naive Bayes classification algorithm.\n","    \"\"\"\n","    def __init__(self):\n","        # Dictionaries of probabilities of words for two labels\n","        self.probs = {\"neutral\": dict(), \"discrim\": dict()}\n","\n","        # Count of words for two labels\n","        self.words = {\"neutral\": 0, \"discrim\": 0}\n","\n","        # Count of tweets for two labels\n","        self.tweets = {\"neutral\": 0, \"discrim\": 0}\n","\n","        # Count of unique words & alpha param\n","        self.unique_words = 0\n","        self.a = 1\n","\n","    def fit(self, tweets: list, labels: list) -> None:\n","        \"\"\"\n","        Fit Naive Bayes parameters according to train data X and y.\n","        :param tweets: pd.DataFrame|list - train input/messages\n","        :param labels: pd.DataFrame|list - train output/labels\n","        :return: None\n","        \"\"\"\n","        def add_tweet_to_dict(tweet, label) -> None:\n","            \"\"\"Fill in dictionaries with all words\"\"\"\n","            other_label = \"discrim\" if label == \"neutral\" else \"neutral\"\n","\n","            for word, count in tweet.items():\n","                if word in self.probs[label].keys():\n","                    self.probs[label][word] += count\n","                    self.words[label] += count\n","                else:\n","                    self.probs[label][word] = count\n","                    self.words[label] += count\n","                    self.unique_words += 1\n","                \n","                if word not in self.probs[other_label].keys():\n","                    self.probs[other_label][word] = 0\n","\n","        def convert_frequency_to_probability(label: str, a: int) -> None: \n","            \"\"\"Convert frequency to probability with param alpha for handling 0 probabilities\"\"\"\n","            words_with_a = self.words[label] + a*self.unique_words\n","\n","            for word, count in self.probs[label].items():\n","                self.probs[label][word] = (count+a) / words_with_a\n","\n","        # Iterate through all tweets and make dictionaries of word frequencies\n","        for tweet, label in zip(tweets, labels):\n","            self.tweets[label] += 1\n","\n","            add_tweet_to_dict(tweet, label)\n","\n","        # Convert frequency dictionaries to probability dictionaries\n","        convert_frequency_to_probability(\"neutral\", self.a)\n","        convert_frequency_to_probability(\"discrim\", self.a)\n","\n","    def predict_prob(self, tweet: str, label: str) -> float:\n","        \"\"\"\n","        Calculate the probability that a given label can be assigned to a given message.\n","        :param tweet: str - input message\n","        :param label: str - label\n","        :return: float - probability P(label|message)\n","        \"\"\"\n","        # Set initial probability to the P(label)\n","        probability = self.tweets[label] / (self.tweets[\"neutral\"] + self.tweets[\"discrim\"])\n","\n","        # Multiply by P(word|label)\n","        for word in tweet.split():\n","            if word in self.probs[label]:\n","                probability *= self.probs[label][word]\n","        \n","        return probability\n","    \n","    def predict(self, tweet: str) -> str:\n","        \"\"\"\n","        Predict label for a given message.\n","        :param message: str - message\n","        :return: str - label that is most likely to be truly assigned to a given message\n","        \"\"\"\n","        # Calculate probability for both labels\n","        prob_discrim = self.predict_prob(tweet, \"discrim\")\n","        prob_neutral = self.predict_prob(tweet, \"neutral\")\n","        \n","        # Assign label too whichever probability is bigger\n","        label = \"discrim\" if max(prob_discrim, prob_neutral) == \"discrim\" else \"neutral\"\n","        return label\n","\n","    def score(self, tweets, correct_labels) -> None:\n","        \"\"\"\n","        Return the mean accuracy on the given test data and labels - the efficiency of a trained model.\n","        :param tweets: pd.DataFrame|list - test data - messages\n","        :param correct_labels: pd.DataFrame|list - test labels\n","        :return:\n","        \"\"\"\n","        accurate = 0\n","        length = len(tweets)\n","\n","        for tweet, correct_label in zip(tweets, correct_labels):\n","            if self.predict(tweet) == correct_label:\n","                accurate += 1\n","        \n","        accuracy = round(accurate / length * 100, 2)\n","        return accuracy\n","            \n","    def __str__(self):\n","        return f\"Word count: {self.words}\\nTweets: {self.tweets}\\nUnique words: {self.unique_words}\\n\"\n"]},{"cell_type":"markdown","metadata":{"id":"GW85An5p-sp3"},"source":["### Testing\n","*  Finally, after you are done with your classifier, test it."]},{"cell_type":"code","execution_count":50,"metadata":{"id":"zo4TZVh4950E"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------\n","model score: 93.0%\n","--------------------\n"]}],"source":["train_X, train_y = process_data(\"./data/train.csv\")\n","test_X, test_y = process_data(\"./data/test.csv\")\n","\n","classifier = BayesianClassifier()\n","classifier.fit(train_X, train_y)\n","classifier.predict_prob(test_X[0], test_y[0])\n","\n","print(\"--\"*10)\n","print(f\"model score: {classifier.score(test_X, test_y)}%\")\n","print(\"--\"*10)"]},{"cell_type":"markdown","metadata":{"id":"JkNytMyM-8n0"},"source":["## Conclusions\n","\n","Summarize your work by explaining in a few sentences the points listed below\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IpEbPLoyBTBA"},"source":["* ### Describe the method implemented in general:\n"]},{"cell_type":"markdown","metadata":{"id":"cAivLQzK_Ksa"},"source":["* ### List pros and cons of the method:"]},{"cell_type":"markdown","metadata":{"id":"IW1pCL-ZBfUD"},"source":["* ### Add a few sencences about your implementation of the classifier:\n"]},{"cell_type":"markdown","metadata":{"id":"ZlIcMe8uBz-2"},"source":["* ### Describe your results:\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"PS_Lab1_Template.ipynb","provenance":[{"file_id":"1Av8y_HRDH36u-pA1zwEWZL9n8GdFvlrF","timestamp":1633697130382},{"file_id":"1aT_CtpODWI7otU6vibcvSxhQp3mXEaP3","timestamp":1633642142132}]},"interpreter":{"hash":"082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"},"kernelspec":{"display_name":"Python 3.8.0 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
